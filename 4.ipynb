{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Same as (2), except using CIFAR dataset instead of MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "0-epoch/total 50, loss: 1.9045\n",
      "1-epoch/total 50, loss: 1.8542\n",
      "2-epoch/total 50, loss: 1.6636\n",
      "3-epoch/total 50, loss: 1.7690\n",
      "4-epoch/total 50, loss: 1.4166\n",
      "5-epoch/total 50, loss: 1.8641\n",
      "6-epoch/total 50, loss: 1.7540\n",
      "7-epoch/total 50, loss: 1.4854\n",
      "8-epoch/total 50, loss: 1.4054\n",
      "9-epoch/total 50, loss: 1.2799\n",
      "10-epoch/total 50, loss: 1.7573\n",
      "11-epoch/total 50, loss: 1.3779\n",
      "12-epoch/total 50, loss: 1.2421\n",
      "13-epoch/total 50, loss: 1.6388\n",
      "14-epoch/total 50, loss: 1.5338\n",
      "15-epoch/total 50, loss: 1.4585\n",
      "16-epoch/total 50, loss: 1.3079\n",
      "17-epoch/total 50, loss: 1.1140\n",
      "18-epoch/total 50, loss: 1.2155\n",
      "19-epoch/total 50, loss: 1.2771\n",
      "20-epoch/total 50, loss: 1.2431\n",
      "21-epoch/total 50, loss: 1.1023\n",
      "22-epoch/total 50, loss: 1.2858\n",
      "23-epoch/total 50, loss: 1.0891\n",
      "24-epoch/total 50, loss: 1.3368\n",
      "25-epoch/total 50, loss: 1.1885\n",
      "26-epoch/total 50, loss: 1.4121\n",
      "27-epoch/total 50, loss: 1.0974\n",
      "28-epoch/total 50, loss: 1.2249\n",
      "29-epoch/total 50, loss: 1.4617\n",
      "30-epoch/total 50, loss: 1.3041\n",
      "31-epoch/total 50, loss: 1.2324\n",
      "32-epoch/total 50, loss: 1.2318\n",
      "33-epoch/total 50, loss: 1.1343\n",
      "34-epoch/total 50, loss: 1.2428\n",
      "35-epoch/total 50, loss: 1.1984\n",
      "36-epoch/total 50, loss: 1.1639\n",
      "37-epoch/total 50, loss: 0.9597\n",
      "38-epoch/total 50, loss: 0.9999\n",
      "39-epoch/total 50, loss: 0.9757\n",
      "40-epoch/total 50, loss: 1.1138\n",
      "41-epoch/total 50, loss: 1.0298\n",
      "42-epoch/total 50, loss: 1.1701\n",
      "43-epoch/total 50, loss: 1.0795\n",
      "44-epoch/total 50, loss: 1.0571\n",
      "45-epoch/total 50, loss: 1.1435\n",
      "46-epoch/total 50, loss: 1.0619\n",
      "47-epoch/total 50, loss: 1.1181\n",
      "48-epoch/total 50, loss: 1.0483\n",
      "49-epoch/total 50, loss: 1.0929\n",
      "Training Time: 232.35\n",
      "Test for train_dataset: correctly matched/total: 30948/50000 accuracy: 61.90%\n",
      "\n",
      "Test for test_dataset: correctly matched/total: 4991/10000 accuracy: 49.91%\n",
      "\n",
      "Number of Parameters:  789258\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim # See https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./cifar_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.CIFAR10(root='./cifar_data/', train=False, transform=transforms.ToTensor(), )\n",
    "\n",
    "'''\n",
    "(1) output shape of model: output is 10 instead of 1\n",
    "(2) Sigmoid --> Softmax\n",
    "(3) BCELoss --> CrossEntropyLoss\n",
    "'''\n",
    "\n",
    "'''\n",
    "Using GPU\n",
    "'''\n",
    "import torch\n",
    "import torch.optim as optim # See https://pytorch.org/docs/stable/optim.html\n",
    "from utils import *\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "'''\n",
    "Define Model\n",
    "'''\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "class MLP(nn.Module) :\n",
    "    def __init__(self, in_dim, out_dim, inter_dim=256) :\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        self.linear1 = nn.Linear(in_dim, inter_dim, bias=True)\n",
    "        #self.act1 = nn.ReLU()        \n",
    "        #self.linear2 = nn.Linear(inter_dim, inter_dim, bias=True)                \n",
    "        #self.act2 = nn.ReLU()        \n",
    "        #self.linear3 = nn.Linear(inter_dim, inter_dim, bias=True)  \n",
    "        self.act3 = nn.ReLU()        \n",
    "        self.linear4 = nn.Linear(inter_dim, out_dim, bias=True)  \n",
    "        \n",
    "    def forward(self, x) :\n",
    "        z = self.linear1(x)\n",
    "        #z = self.act1(z)\n",
    "        #z = self.linear2(z)\n",
    "        #z = self.act2(z)\n",
    "        #z = self.linear3(z)  \n",
    "        z = self.act3(z)   \n",
    "        z = self.linear4(z)\n",
    "        return z\n",
    "\n",
    "'''\n",
    "Choose loss function\n",
    "'''\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "loss = torch.nn.CrossEntropyLoss() # (2) Change to CrossEntropyLoss\n",
    "\n",
    "\n",
    "''' Define model and optimizer'''\n",
    "MLP_model = MLP(32*32*3, 10) # (4) change input/output shape\n",
    "# Reverse mode automatic differentiation\n",
    "OPTIMIZER = optim.SGD(MLP_model.parameters(),  lr=0.1)\n",
    "\n",
    "'''\n",
    "DataLoader is used to apply minibatch SGD\n",
    "'''\n",
    "batch_size = 128\n",
    "image_size = 32*32*3\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "MLP_model.to(device)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "num_epoch = 50\n",
    "for epoch in range(num_epoch):\n",
    "    for image,label in train_loader :\n",
    "\n",
    "        # Forward process\n",
    "        hat_y = MLP_model(image.view(-1, 32*32*3).to(device))\n",
    "        cost = loss(hat_y, label.to(device))\n",
    "\n",
    "        # Wipe up gradient in the previous step\n",
    "        OPTIMIZER.zero_grad() \n",
    "\n",
    "        # Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "        cost.backward() \n",
    "\n",
    "        # Updates the parameters\n",
    "        OPTIMIZER.step()\n",
    "    \n",
    "    print(\"{}-epoch/total {}, loss: {:.4f}\".format(epoch, num_epoch, cost))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Training Time: {:.2f}\".format(end-start))\n",
    "\n",
    "\n",
    "\n",
    "MLP_model.to(\"cpu\")\n",
    "\n",
    "''' Test for train_dataset '''\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "count = 0\n",
    "for image,label in train_loader :\n",
    "    label = label\n",
    "    prediction = torch.argmax(MLP_model(image.view(len(image),3*32*32)),dim=1)\n",
    "    count += (prediction == label).sum()\n",
    "        \n",
    "print('Test for train_dataset: correctly matched/total: {}/{} accuracy: {:.2f}%\\n'.format(count, len(train_dataset.targets), count/len(train_dataset.targets) * 100. ) )    \n",
    "\n",
    "''' Test for test_dataset '''\n",
    "count = 0\n",
    "wrong_answer = []\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "for image,label in test_loader :\n",
    "    label = label\n",
    "    prediction = torch.argmax(MLP_model(image.view(len(image),3*32*32)),dim=1)\n",
    "    count += (prediction == label).sum()\n",
    "    \n",
    "#     else:\n",
    "#         wrong_answer += [i]   \n",
    "        \n",
    "print('Test for test_dataset: correctly matched/total: {}/{} accuracy: {:.2f}%\\n'.format(count, len(test_dataset.targets), count/len(test_dataset.targets) * 100. ) )    \n",
    "\n",
    "total_params = sum(p.numel() for p in MLP_model.parameters())\n",
    "print(\"Number of Parameters: \", total_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Test Code ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test for test_dataset: correctly matched/total: 4991/10000 accuracy: 49.91%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Test for test_dataset '''\n",
    "count = 0\n",
    "wrong_answer = []\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "for image,label in test_loader :\n",
    "    label = label\n",
    "    prediction = torch.argmax(MLP_model(image.view(len(image),image_size)),dim=1)\n",
    "    count += (prediction == label).sum()\n",
    "    \n",
    "#     else:\n",
    "#         wrong_answer += [i]   \n",
    "        \n",
    "print('Test for test_dataset: correctly matched/total: {}/{} accuracy: {:.2f}%\\n'.format(count, len(test_dataset.targets), count/len(test_dataset.targets) * 100. ) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Review ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layer 수를 2개이상으로 올리니 정확도 미묘하게 내려감.\n",
    "### batch_size를 128이상으로 올리니 정확도 내려감.\n",
    "### (batch_size를 올린 만큼 epoch 수를 올려야 하지만 학습 시간이 오래걸려 test 불가)\n",
    "### batch = 128, epoch = 50으로 마무리\n",
    "### inter_dim 각각 256이상, 미만은 정확도 내려감"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
